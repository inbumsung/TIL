{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Course (part 2)\n",
    "\n",
    "- This document summarizes machine learning introduction from [Machine learning course from Kaggle](https://www.kaggle.com/learn/machine-learning).\n",
    "- You can download data required for the following exercises through [Kaggle API](https://github.com/Kaggle/kaggle-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading melbourne-housing-snapshot.zip to ./data_files\n",
      "100%|████████████████████████████████████████| 451k/451k [00:00<00:00, 1.14MB/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download sample data\n",
    "!kaggle datasets download --path ./data_files --unzip dansbecker/melbourne-housing-snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "Most libraries (including scikit-learn) will give you an error if you try to build a model using data with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Basic problem set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset \n",
    "melb_data = pd.read_csv('./data_files/melb_data.csv')\n",
    "\n",
    "melb_target = melb_data.Price\n",
    "melb_predictors = melb_data.drop(['Price'], axis='columns')\n",
    "\n",
    "# for the sake of keeping the example simple, we'll use only numeric predictors.\n",
    "melb_numeric_predictors = melb_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "# divide our data into training and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(melb_numeric_predictors,\n",
    "                                                                        melb_target,\n",
    "                                                                        train_size = 0.7,\n",
    "                                                                        test_size = 0.3,\n",
    "                                                                        random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Create function to measure quality of different approaches to missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def score_dataset(x_train, x_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(random_state=1)\n",
    "    model.fit(x_train, y_train)\n",
    "    preds = model.predict(x_test)\n",
    "    return mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Get model score from dropping columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from dropping columns with missing values : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inbumsung/workspace/venv/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191833.53711690864\n"
     ]
    }
   ],
   "source": [
    "cols_with_missing = [col for col in x_train.columns if x_train[col].isnull().any()]\n",
    "reduced_x_train = x_train.drop(cols_with_missing, axis='columns')\n",
    "reduced_x_test = x_test.drop(cols_with_missing, axis='columns')\n",
    "\n",
    "print('Mean Absolute Error from dropping columns with missing values : ')\n",
    "print(score_dataset(reduced_x_train, reduced_x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Get model score from imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from imputation : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inbumsung/workspace/venv/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182349.8471234542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "# re-cast imputed results to Pandas DataFrame \n",
    "imputed_x_train = pd.DataFrame(my_imputer.fit_transform(x_train))\n",
    "imputed_x_test = pd.DataFrame(my_imputer.transform(x_test))\n",
    "imputed_x_train.columns = x_train.columns\n",
    "imputed_x_test.columns = x_test.columns\n",
    "\n",
    "print('Mean Absolute Error from imputation : ')\n",
    "print(score_dataset(imputed_x_train, imputed_x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### fit_transform vs. transform\n",
    "fit_transform means to do some calculation (normally calculate the means of colums) and then do transformation (replacing the missing values). So for training set, you need to both calcuate and do transformation.\n",
    "But for testing set which will be less in number, it is intuitive to use mean values derived from larger training data. So it doesn't need to calculate, it just performs the transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Get model score from imputation with extra columns showing what was imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error from imputation while tracking what was imputed: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inbumsung/workspace/venv/lib/python3.7/site-packages/sklearn/ensemble/forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182182.084975571\n"
     ]
    }
   ],
   "source": [
    "# make copy to avoid changing original data \n",
    "x_train_plus = x_train.copy()\n",
    "x_test_plus = x_test.copy()\n",
    "\n",
    "cols_with_missing = [col for col in x_train.columns if x_train[col].isnull().any()]\n",
    "\n",
    "# make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    x_train_plus[col + '_was_missing'] = x_train_plus[col].isnull()\n",
    "    x_test_plus[col + '_was_missing'] = x_test_plus[col].isnull()\n",
    "    \n",
    "my_imputer = SimpleImputer()\n",
    "imputed_x_train_plus = pd.DataFrame(my_imputer.fit_transform(x_train_plus))\n",
    "imputed_x_test_plus = pd.DataFrame(my_imputer.transform(x_test_plus))\n",
    "imputed_x_train_plus.columns = x_train_plus.columns\n",
    "imputed_x_test_plus.columns = x_test_plus.columns\n",
    "\n",
    "print('Mean Absolute Error from imputation while tracking what was imputed: ')\n",
    "print(score_dataset(imputed_x_train_plus, imputed_x_test_plus, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using categorical data with one hot encoding\n",
    "Categorical data is data that takes only a limited number of values. You will get an error if you tro to plug these variables into most machine learning models in Python wihtout \"encoding\" them first. Here we'll see the most popular method for encoding categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',\n",
       "       'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n",
       "       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n",
       "       'Longtitude', 'Regionname', 'Propertycount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melb_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.csv to ./data_files\n",
      "100%|█████████████████████████████████████████| 450k/450k [00:01<00:00, 289kB/s]\n",
      "\n",
      "Downloading test.csv to ./data_files\n",
      "100%|████████████████████████████████████████| 441k/441k [00:00<00:00, 1.06MB/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "!kaggle competitions download -c house-prices-advanced-regression-techniques --path ./data_files --file train.csv\n",
    "!kaggle competitions download -c house-prices-advanced-regression-techniques --path ./data_files --file test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Basic problem set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "train_data = pd.read_csv('./data_files/train.csv')\n",
    "test_data = pd.read_csv('./data_files/test.csv')\n",
    "\n",
    "# drop houses where the target is missing\n",
    "train_data.dropna(axis='rows', subset=['SalePrice'], inplace=True)\n",
    "\n",
    "target = train_data.SalePrice\n",
    "\n",
    "# since missing values isn't the focus of this tutorial, we use the simplest\n",
    "# possible approach, which drops these columns. \n",
    "cols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]                                  \n",
    "candidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis='columns')\n",
    "candidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis='columns')\n",
    "\n",
    "# \"cardinality\" means the number of unique values in a column.\n",
    "# we use it as our only way to select categorical columns here. This is convenient, though\n",
    "# a little arbitrary.\n",
    "low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].nunique() < 10 and\n",
    "                                candidate_train_predictors[cname].dtype == \"object\"]\n",
    "numeric_cols = [cname for cname in candidate_train_predictors.columns if \n",
    "                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "my_cols = low_cardinality_cols + numeric_cols\n",
    "\n",
    "train_predictors = candidate_train_predictors[my_cols]\n",
    "test_predictors = candidate_test_predictors[my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : get one-hot encodings using `get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataframe has Street column which has two unique values : ['Pave', 'Grvl']\n",
      "one-hot encoded dataframe has two columns derived from Street column as follows : \n",
      "   Street_Pave  Street_Grvl\n",
      "0            1            0\n",
      "1            1            0\n",
      "2            1            0\n",
      "3            1            0\n",
      "4            1            0\n"
     ]
    }
   ],
   "source": [
    "# by default, get_dummies would only create dummy variables for dtype=object\n",
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "\n",
    "print('original dataframe has Street column which has two unique values : {}'.format(train_predictors.Street.unique().tolist()))\n",
    "print('one-hot encoded dataframe has two columns derived from Street column as follows : ')\n",
    "print(one_hot_encoded_training_predictors[['Street_Pave', 'Street_Grvl']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Compare MAE with two different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inbumsung/workspace/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/Users/inbumsung/workspace/venv/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error when Dropping Categoricals: 18584\n",
      "Mean Abslute Error with One-Hot Encoding: 18288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def get_mae(x, y):\n",
    "    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n",
    "    return -1 * cross_val_score(RandomForestRegressor(50), x, y, scoring = 'neg_mean_absolute_error').mean()\n",
    "\n",
    "predictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n",
    "\n",
    "mae_without_categoricals = get_mae(predictors_without_categoricals, target)\n",
    "\n",
    "mae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n",
    "\n",
    "print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\n",
    "print('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Applying to multiple files\n",
    "Scikit-learn is sensitive to the ordering of columns, so if the training dataset and test dataset gets misaligned, your results will be nonsense. This could happen if a categorical had a different number of values in the training data vs the test data. \n",
    "Ensure the test data is encoded in the same manner as the training data with the `align` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\n",
    "one_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\n",
    "final_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors, join='left', axis='columns')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
